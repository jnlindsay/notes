\section{Linear algebra}

\subsection{Matrices}

\subsubsection{Inverse matrices}

\begin{definition}{Inverse matrix \cite{math1141_notes}}{inverse_matrix}
A matrix $X$ is said to be the \textbf{inverse} of a matrix $A$ if both
$$ AX = I \qquad \text{and} \qquad XA = I , $$
where $I$ is the identity matrix of appropriate size.
\end{definition}

If a matrix $A$ has an inverse, then $A$ is said to be an \textbf{invertible matrix}. If a matrix $A$ is not an invertible matrix, then it is called a \textbf{singular} matrix.

\begin{lemma}{\cite{math1141_notes}}{}
All invertible matrices are square.
\end{lemma}

\begin{lemma}{}{singular_matrix}
A (square) matrix is singular if its determinant is $0$.
\end{lemma}

\begin{definition}{Right/left inverse \cite{math1141_notes}}{right_left_inverse}
\begin{itemize}
	\item An $n \times m$ matrix $X$ is said to be a \textbf{right inverse} of the $m \times n$ matrix $A$ if
$$ AX = I_m ; $$

	\item An $n \times m$ matrix $Y$ is said to be a \textbf{left inverse} of the $m \times n$ matrix $A$ if
$$ YA = I_n . $$
\end{itemize}
\end{definition}

\textbf{Finding the inverse of a matrix}. A matrix $A$ is invertible if and only if it can be reduced by elementary row operations to an identity matrix $I$ and if $(A|I)$ can be reduced to $(I|B)$, in which case, $B = A^{-1}$.

\begin{formula}{Inverse of a $2 \times 2$ matrix}{inverse_2b2_matrix}
$$
\begin{pmatrix}
	a & b \\
	c & d
\end{pmatrix}^{-1}
= \dfrac{1}{ad - bc}
\begin{pmatrix}
	d & -b \\
	-c & a
\end{pmatrix} , \quad
\text{provided } ad - bc \not = 0.
$$
\end{formula}

\subsubsection{Determinants}

\begin{definition}{Determinant of a $2 \times 2$ matrix}{determinant_2b2}
Given the matrix
$$ A = \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{pmatrix} , $$
its \textbf{determinant} is
$$ \det (A) = a_{11} a_{22} - a_{12} a_{21} . $$
\end{definition}

\begin{definition}{Determinant}{determinant}
First, define the row $i$, column $j$ \textbf{minor} of a matrix $X$ to be the resulting matrix obtained by deleting row $i$ and column $j$ from $X$, denoted by $ \lvert X_{ij} \rvert$.

Then the \textbf{determinant} of an $n \times n$ matrix $A$ is
\begin{align*}
\lvert A \rvert &=
a_{11} \lvert A_{11} \rvert 
- a_{12} \lvert A_{12} \rvert
+ a_{13} \lvert A_{13} \rvert
- \dots +
(-1)^{1+n} a_{1n} \lvert A_{1n} \rvert \\
&= \sum_{k = 1}^n (-1)^{1+k} a_{1k} \lvert A_{1k} \rvert .
\end{align*}
\end{definition}

\textbf{Properties of determinants} \cite{math1141_notes}:
\begin{enumerate}
	\item $\det (A) = \det (A^T)$
	\item If any two rows (or any two columns) of $A$ are interchanged, then the sign of the determinant is reversed.
	\item If a matrix contains a zero row or column then its determinant is zero.
	\item If a row (or column) of $A$ is multiplied by a scalar, then the value of $\det A$ is multiplied by the same scalar.
	\item If any column of a matrix is a multiple of another column of the matrix (or any row is a multiple of another row), then the value of $\det (A)$ is zero.
	\item If a multiple of one row (or column) is added to another row (or column), then the value of the determinant is not changed.
	\item If $A$ and $B$ are square matrices such that the product $AB$ exists, then
$$ \det (AB) = \det (A) \det (B) . $$
\end{enumerate}

\subsection{Groups and fields}

\begin{definition}{Group \cite{math2601_notes}}{group}
	A \textbf{group} is a a set, $G$, together with an operation $*$ (called the \textbf{group law} of $G$) that combines any two elements $a$ and $b$ to form another element, denoted $a * b$ or $ab$. To qualify as a group, the set and operation, $(G, *)$, must satisfy four requirements known as the \textbf{group axioms}:
	\begin{itemize}
		\item \textbf{Closure.} For all $a, b$ in $G$, the result of the operation, $a * b$, is also in $G$.
		\item \textbf{Associativity.} For all $a, b$ and $c$ in $G, (a * b) * c = a * (b * c)$.
		\item \textbf{Identity element.} There exists an element $e$ in $G$ such that, for every element $a$ in $G$, the equation $e * a = a * e = a$ holds. Such an element is unique, and thus one speaks of \textit{the} identity element.
		\item \textbf{Inverse element.} For each $a$ in $G$, there exists an element $b$ in $G$, commonly denoted $a^{-1}$ (or $-a$, if the operation is denoted ``$+$''), such that $a * b = b * a = e$, where $e$ is the identity element.
	\end{itemize}
	If $G$ is a finite set then the \textbf{order} of $G$ is $\lvert G \rvert$, the number of elements in $G$.
\end{definition}

A group $G$ is \textbf{abelian} if the operation satisfies the \textbf{commutative law}
$$ a * b = b * a \qquad \text{for all} \ a, b \in G . $$

We often write the \textbf{cyclic group of order $m$} as
$$ C_m = \langle a : a^m = e \rangle , $$
and say it is \textit{generated} by $a$.

\begin{definition}{Subgroup \cite{math2601_notes}}{subgroup}
Let $(G, *)$ be a group and $H$ a non-empty subset of $G$. If $H$ is a group under the restriction of $*$ to $H$, we call it a \textbf{subgroup} of $G$. We write this as $H \leq G$ and say $H$ \textit{inherits} the group structure from $G$.
\end{definition}

\begin{lemma}{\cite{math2601_notes}}{subgroup}
Let $(G, *)$ be a group and $H$ a non-empty subset of $G$. Then $H$ is a subgroup of $G$ if and only if
\begin{itemize}
	\item for all $a, b \in H, \ a * b \in H$;
	\item for all $a \in H, \ a^{-1} \in H$,
\end{itemize}
i.e. $H$ is closed under $*$ and $^{-1}$.
\end{lemma}

\begin{definition}{Field \cite{math2601_notes}}{field}
A \textbf{field}, $(\mathbb{F}, +, \times)$ is a set $\mathbb{F}$ with two binary operations on it --- addition ($+$) and multiplication ($\times$) --- where
\begin{enumerate}
	\item $(\mathbb{F}, +)$ is an abelian group,
	\item $\mathbb{F}^{*} = \mathbb{F} \setminus \{0\}$ (where 0 is the additive identity) is an abelian group under multiplication,
	\item the distributive laws $a \times (b + c) = a \times b + a \times c$ and $(a + b) \times c = a \times c + b \times c$ hold.
\end{enumerate}
\end{definition}

\begin{definition}{Subfield \cite{math2601_notes}}{subfield}
If $(\mathbb{F}, +, \times)$ is a field and $\mathbb{E} \subset \mathbb{F}$ is also a field under the same operations (restricted to $\mathbb{E}$), then $(\mathbb{E}, +, \times)$ is a \textbf{subfield} of $(\mathbb{F}, +, \times)$, usually written $\mathbb{E} \leq \mathbb{F}$.
\end{definition}

\begin{lemma}{\cite{math2601_notes}}{subfield}
Let $\mathbb{E} \not = \{0\}$ be a non-empty subset of field $\mathbb{F}$. Then $\mathbb{E}$ is a subfield of $\mathbb{F}$ if and only if for all $a, b \in \mathbb{E}$:
$$ a + b \in \mathbb{E}, \quad -b \in \mathbb{E}, \quad a \times b \in \mathbb{E}, \quad b^{-1} \in \mathbb{E} \text{ if } b \not = 0 . $$

\begin{proof}
The distributive laws are inherited from $\mathbb{F}$ to $\mathbb{E}$. The rest of the proof follows from applying the subgroup lemma \ref{lem:subgroup} to both $(\mathbb{E}, +)$ and $(\mathbb{E}^*, \times)$.
\end{proof}
\end{lemma}

\begin{definition}{General linear group \cite{math2601_notes}}{general_linear_group}
Let $n \geq 1$ be any integer. The \textbf{general linear group}, $\operatorname{GL}(n, \mathbb{F})$ is the set of invertible $n \times n$ matrices over field $\mathbb{F}$ under matrix multiplication, and is non-abelian if $n > 1$.
\end{definition}

Subgroups of $\operatorname{GL}(n, \mathbb{F})$ include
\begin{itemize}
	\item the \textbf{special linear groups} $\operatorname{SL}(n, \mathbb{R})$ and $\operatorname{SL}(n, \mathbb{C})$ of matrices with determinant 1;
	\item $\operatorname{O}(n) \leq \operatorname{GL}(n, \mathbb{R})$, the group of \textbf{orthogonal matrices};
	\item $\operatorname{SO}(n) = \operatorname{O}(n) \cap \operatorname{SL}(n, \mathbb{R})$, the group of \textbf{special orthogonal matrices}.
\end{itemize}

\begin{definition}{Group homomorphism \cite{math2601_notes}}{group_homomorphism}
Let $(G, *)$ and $(H, \circ)$ be two groups. A \textbf{(group) homomorphism} from $G$ to $H$ is a map $\phi : G \to H$ that respects the two operations, that is where
$$ \phi (a * b) = \phi (a) \circ \phi (b) \qquad \text{for all } a, b \in G . $$
A bijective homomorphism $\phi : G \to H$ is called an \textbf{isomorphism}: the groups are then said to be \textbf{isomorphic}.
\end{definition}

\begin{lemma}{\cite{math2601_notes}}{homomorphism}
Let $(G, *)$ and $(H, \circ)$ be two groups and $\phi$ a homomorphism between them. Then
\begin{itemize}
	\item $\phi$ maps the identity of $G$ to the identity of $H$;
	\item $\phi$	 maps inverse to inverses, i.e. $\phi (a^{-1}) = (\phi(a))^{-1}$ for all $a \in G$;
	\item if $\phi$ is an isomorphism from $G$ to $H$ then $\phi^{-1}$ is an isomorphism from $H$ to $G$.
\end{itemize}
\end{lemma}

\begin{definition}{Kernel and image \cite{math2601_notes}}{kernel_and_image}
Let $\phi : G \to H$ be a group homomorphism, with $e'$ the identity of $H$.

The \textbf{kernel} of $\phi$ is the set
$$ \ker (\phi) = \{ g \in G : \phi (g) = e' \} . $$

The \textbf{image} of $\phi$ is the set
$$ \operatorname{im} (\phi) = \{ h \in H : h = \phi (g), \text{ some } g \in G \} . $$
\end{definition}

\begin{lemma}{\cite{math2601_notes}}{kernel_image_subgroups}
For $\phi : G \to H$, a group homomorphism, $\ker \phi \leq G$ and $\operatorname{im} \phi \leq H$.

\begin{proof}
Let $e$ be the identity of $G$. From lemma \ref{lem:homomorphism}, $e \in \ker \phi$, so the kernel is non-empty.

If $a, b \in \ker \phi$ then
$$ \phi (a * b) = \phi (a) \circ \phi (b) = e' \circ e' = e' , $$
so $a * b \in \ker \phi$.

If $a \in \ker \phi$ then from lemma \ref{lem:homomorphism},
$$ \phi (a^{-1}) = (\phi (a))^{-1} = (e')^{-1} = e' , $$
and so $a^{-1} \in \ker \phi$.

Thus by the subgroup lemma \ref{lem:homomorphism}, $\ker \phi \leq G$.

\textit{The proof that $\operatorname{im} \phi \leq H$ is omitted.}
\end{proof}
\end{lemma}

\begin{lemma}{\cite{math2601_notes}}{}
A group homomorphism $\phi : G \to H$ is one-to-one if and only if $\ker \phi = \{e\}$, with $e$ the identity of $G$; if $\phi$ is one-to-one then $\operatorname{im} \phi$ is isomorphic to $G$.

\begin{proof}
From lemma \ref{lem:kernel_image_subgroups}, $e \in \ker \phi$, and if $\phi$ is one-to-one, $e$ is the only element that maps to $e'$, the identity of $H$.

Conversely, suppose $\ker \phi = \{e\}$ and $\phi (a) = \phi (b)$, where $a, b \in G$. Then
$$ \phi (a * b^{-1}) = \phi (a) \circ (\phi (b))^{-1} = e' $$
and so $a * b^{-1} = e$ and $a = b$.

If $\phi$ is one-to-one, it is a bijection from $G$ to $\operatorname{im} \phi$, and hence an isomorphism.
\end{proof}
\end{lemma}

A common use of group homomorphisms is to look for a homomorphism $\phi : G \to \operatorname{GL}(n, \mathbb{F})$ for some $n$ and some field $F$. The group $\operatorname{im} (\phi)$ is called a \textbf{(linear) representation of $G$ on $\mathbb{F}^n$}.

If $\phi$ is one-to-one (so every element maps to a distinct matrix), we call the representation \textbf{faithful}.

\subsection{Vector spaces}

\begin{definition}{Vector space \cite{math2601_notes}}{vector_space}
Let $\mathbb{F}$ be a field. A \textbf{vector space over the field} $\mathbb{F}$ consists of an abelian group $(V, +)$ plus a function from $\mathbb{F} \times V$ to $V$ called \textbf{scalar multiplication} and written $\alpha \mathbf{v}$, where
\begin{itemize}
	\item $\alpha (\beta \mathbf{v}) = (\alpha \beta) \mathbf{v}$ for all $\alpha, \beta \in \mathbb{F}$ for all $\mathbf{v} \in V$.
	\item $1 \mathbf{v} = \mathbf{v}$ for all $\mathbf{v} \in V$.
	\item $\alpha (\mathbf{u} + \mathbf{v}) = \alpha \mathbf{u} + \alpha \mathbf{v}$ for all $\alpha \in \mathbb{F}$ and for all $\mathbf{u}, \mathbf{v} \in V$.
	\item $(\alpha + \beta) \mathbf{u} = \alpha \mathbf{u} + \beta \mathbf{u}$ for all $\alpha, \beta \in \mathbb{F}$ for all $\mathbf{u} \in V$.
\end{itemize}
\end{definition}

\begin{lemma}{\cite{math2601_notes}}{}
Let $V$ be a vector space over field $\mathbb{F}$. For all $\mathbf{v}, \mathbf{w} \in V$ and $\lambda \in \mathbb{F}$:
\begin{itemize}
	\item $0 \mathbf{v} = \mathbf{0}$ and $\lambda \mathbf{0} = \mathbf{0}$;
	\item $(-1) \mathbf{v} = - \mathbf{v}$;\
	\item $\lambda \mathbf{v} = \mathbf{0}$ implies either $\lambda = 0$ or $\mathbf{v} = \mathbf{0}$;
	\item if $\lambda \mathbf{v} = \lambda \mathbf{w}$ and $\lambda \not = 0$ then $\mathbf{v} = \mathbf{w}$.
\end{itemize}
\end{lemma}

\subsubsection{Subspaces}

\begin{definition}{Linear (vector) subspace \cite{math2601_notes}}{linear_subspace}
If $V$ is a vector space over $\mathbb{F}$ and $U \subseteq V$, then $U$ is a \textbf{subspace} of $V$, written $U \leq V$, if it is a vector space over $\mathbb{F}$ with the same addition and scalar multiplication as in $V$.
\end{definition}

\begin{lemma}{(Linear) subspace test}{subspace_test}
Suppose $V$ is a vector space over the field $\mathbb{F}$ and $U$ is a non-empty subset of $V$. Then $U$ is a subspace of $V$ if and only if for all $\mathbf{u}, \mathbf{v}, \in U$ and $\alpha \in \mathbb{F}$, it holds that $\alpha \mathbf{u} + \mathbf{v} \in U$.

\begin{proof}
Most of the axioms will be simply inherited from $V$: all we really need to prove is closure and that the zero vector is in $U$.

But setting $\alpha = 1$ proves $U$ is closed under addition.

With $\alpha = -1$ and $\mathbf{v} = \mathbf{u}$ we get $\mathbf{0} \in U$.

Hence we can set $\mathbf{v} = \mathbf{0}$ to get closure under scalar multiplication.

``The rest is easy.''
\end{proof}
\end{lemma}

Examples:
\begin{itemize}
	\item Every vector space has ${\mathbf{0}}$ (the \textbf{trivial subspace}) and itself as subspaces.
	\item Polynomials and functions: $\mathcal{P}_n(\mathbb{R})$ is a subspace of $\mathcal{P}(\mathbb{R})$, which is a subspace of $\mathcal{R}[\mathbb{R}]$.
\end{itemize}

\subsubsection{Linear combinations, spans and independence}

\begin{definition}{Linear combination \cite{math2601_notes}}{linear_combination}
Let $V$ be a vector space over $\mathbb{F}$. A \textbf{(finite) linear combination} of vectors $\mathbf{v}_1 + \mathbf{v}_2, \ldots, \mathbf{v}_n$ in $V$ is any vector which can be expressed
$$ \alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \dots + \alpha_n \mathbf{v}_n , $$
where the $\alpha_k$ are scalars.
\end{definition}

\begin{definition}{Span \cite{math2601_notes}}{span}
If $S$ is a subset of $V$, then the \textbf{span} of $S$ is
$$ \operatorname{span}(S) = \{ \text{all finite linear combinations of vectors in } S \} . $$
If $\operatorname{span}(S) = V$, we say that $S$ \textbf{spans} $V$, or \textbf{is a spanning set for} $V$.
\end{definition}

\begin{lemma}{\cite{math2601_notes}}{}
If $S$ is a non-empty subset of a vector space $V$, then $\operatorname{span}(S)$ is a subspace of $V$.

\begin{proof}
Since $S \subseteq \operatorname{span}(S)$, we know that $\operatorname{span}(S)$ is non-empty.

But if $\mathbf{v}, \mathbf{w} \in \operatorname{span}(S)$, then $\lambda \mathbf{v} + \mathbf{w}$ is a linear combination of two linear combinations of elements of $S$, and so is a linear combination of elements of $S$.

Thus $\lambda \mathbf{v} + \mathbf{w} \in \operatorname{span}(S)$ and $\operatorname{span}(S) \leq V$.
\end{proof}
\end{lemma}

\begin{definition}{Linear independence \cite{math2601_notes}}{linear_independence}
A subset $S$ of a vector space $V$ is \textbf{linearly independent} if for all vectors $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n$ in $S$ (with $n \geq 1$) the equation
$$ \alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \dots + \alpha_n \mathbf{v}_n = \mathbf{0} , $$
with $\alpha_i \in \mathbb{F}$, implies $\alpha_i = 0$ for all $i = 1 \ldots n$.

A set which is not linearly independent is said to be \textbf{linearly dependent}.
\end{definition}

\begin{lemma}{Linear dependence \cite{math2601_notes}}{linear_dependence}
If $S = \{ \mathbf{v}_1, \ldots, \mathbf{v}_n \}$ is a linearly dependent set in $V$, then there is an $i$ where $2 \leq i \leq n$ such that
$$ \mathbf{v}_i = \sum_{j = 1}^{i - 1} \beta_j \mathbf{v}_j , $$
where $\beta_j \in \mathbb{F}$ for $j = 1 \ldots i - 1$.

In other words, in an ordered linearly dependent set, at least one vector is a linear combination of its predecessors.
\end{lemma}

\subsubsection{Bases}

\begin{definition}{Basis \cite{math2601_notes}}{basis}
Let $S \subseteq V$. The set $S$ is a \textbf{basis} for $V$ over $\mathbb{F}$ if and only $V = \operatorname{span}(S)$, and $S$ is a linearly independent set.
\end{definition}

\subsubsection{Dimension}

\begin{theorem}{Existence of finite basis \cite{math2601_notes}}{existence_finite_basis}
Let $V$ be a vector space over $\mathbb{F}$ and $S$ a finite set that spans $V$. Then $S$ contains a finite basis for $V$.
\end{theorem}

\begin{lemma}{Exchange Lemma \cite{math2601_notes}}{exchange}
Suppose that $S$ is a finite spanning set for $V$ and that $T$ is a (finite) linearly independent subset of $V$ with $\lvert T \rvert \leq \lvert S \rvert$. Then there is a spanning set $S'$ of $V$ such that
$$ T \subseteq S' \quad \text{and} \quad \lvert S' \rvert = \lvert S \rvert . $$
\end{lemma}

\begin{corollary}{\cite{math2601_notes}}{size_independent_spanning}
If $S$ is a finite spanning set for a vector space $V$, and $T$ is a linearly independent subset of $V$, then $T$ is finite and $\lvert T \rvert \leq \lvert S \rvert$.

In other words, \textit{independent sets are no larger than spanning sets (if there is a finite spanning set}.

\begin{proof}
Suppose that $\lvert T \rvert > \lvert S \rvert$, and consider any subset $T_0$ of $T$ with exactly $\lvert S \rvert$ elements.

Let $\mathbf{v}$ be an element of $T \setminus T_0$.

Applying the Exchange Lemma (lemma \ref{lem:exchange}) to $T_0$ and $S$ gives a spanning set $S'$ for which
$$ T_0 \subseteq S' \quad \text{and} \quad \lvert S' \rvert = \lvert T_0 \rvert . $$
But this implies that $T_0 = S'$ and so $T_0$ is a spanning set for $V$.

But then $v$ is in $\operatorname{span}(T_0)$, implying that $T$ is linearly dependent. This is a contradiction, so in fact $\lvert T \rvert \leq \lvert S \rvert$.
\end{proof}
\end{corollary}

\begin{theorem}{\cite{math2601_notes}}{}
Let $V$ be a vector space over $\mathbb{F}$ with a finite spanning set and $T$ a linearly independent subset of $V$. Then there is a basis $B$ of $V$ which contains $T$.

In other words, \textit{any linearly independent set can be extended to a basis (if there is a finite spanning set)}.

\begin{proof}
By the Exchange Lemma (lemma \ref{lem:exchange}), there is a finite spanning set $S'$ with $T \subseteq S'$.

Consider the (non-empty) collection of all the linearly independent subsets of $S'$ that contain $T$.

This collection must be finite (as $S'$ is), so there must be least one that is maximal; let $B$ be one such.

But for every $\mathbf{v} \in S'$, the set $B \cup \{\mathbf{v}\}$ is linearly dependent (by maximality of $B$) and so $\mathbf{v} \in \operatorname{span}(B)$. Therefore
$$ V = \operatorname{span}(S') \subseteq \operatorname{span}B) . $$
So $B$, which by construction contains $T$, is a basis for $V$.
\end{proof}
\end{theorem}

\begin{theorem}{Size of all bases \cite{math2601_notes}}{size_all_bases}
If vector space $V$ admits a finite spanning set, it admits a finite basis and all bases contain the same number of elements.

\begin{proof}
By theorem \ref{th:existence_finite_basis}, $V$ must contain at least one finite basis, $\mathcal{B}_1$, say.

Suppose that $\mathcal{B}_2$ is a second basis for $V$.

Since $\mathcal{B}_2$ is linearly independent and $\mathcal{B}_1$ spans $V$, corollary \ref{cor:size_independent_spanning} says that $\lvert \mathcal{B}_2 \rvert \leq \lvert \mathcal{B}_1 \rvert$, so $\mathcal{B}_2$ is also finite.

But then similarly, $\lvert \mathcal{B}_1 \rvert \leq \lvert \mathcal{B}_2 \rvert$, and this completes the proof.
\end{proof}
\end{theorem}

\begin{definition}{Dimension \cite{math2601_notes}}{dimension}
The \textbf{dimension} of a vector space $V$ is the size of a basis if $V$ has a finite basis or infinity otherwise.

The notation is $\dim (V) = n$ or $\dim(V) = \infty$.

If we need to emphasis the field, we use the notation $\dim_\mathbb{F}(V)$.
\end{definition}

\begin{lemma}{\cite{math2601_notes}}{}
Let $V$ be a finite dimensional vector space and suppose $\dim (V) = n$.
\begin{enumerate}[a)]
	\item The number of elements in any spanning set is at least $n$.
	\item The number of elements in any independent set is no more than $n$.
	\item If $\operatorname{span} (S) = V$ and $\lvert S \rvert = n$ then $S$ is a basis.
	\item If $S$ is a linearly independent set and $\lvert S \rvert = n$ then $S$ is a basis.
\end{enumerate}
\end{lemma}

\begin{theorem}{\cite{math2601_notes}}{basis_written_uniquely}
	Let $V$ be a finite dimensional vector space over $\mathbb{F}$.

	Then $\mathcal{B} = \{ \mathbf{v}_1, \ldots, \mathbf{v}_n \}$ is a basis for $V$ if and only if every $\mathbf{x} \in V$ can be written uniquely as
	$$ \mathbf{x} = \sum_{i = 1}^{n} \alpha_i \mathbf{v}_i, \quad \alpha_i \in \mathbb{F} $$

	\begin{proof}
		Suppose that $\lvert S \rvert = q$; we shall prove the result by induction on $\lvert T \rvert = p \leq q$.

		For the base case $p = 0$, simply choose $S' = S$.

		Now suppose that the result is true for some specific non-negative integer $p < q$, and consider a linearly independent set
		$$ T = \{ \mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_{p+1} \} . $$

		As $\{ \mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_{p} \}$ is also linearly independent, there is a set
		$$ S_1 = \{ \mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_{p}, \mathbf{v}_{p+1}, \ldots, \mathbf{v}_q \} $$
		that spans $V$ by our inductive hypothesis. Thus
		$$ S_2 = \{ \mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_{p}, \mathbf{u}_{p+1}, \mathbf{v}_{p+1}, \ldots, \mathbf{v}_q \} $$
		is a linearly \textit{dependent} spanning set of $V$ (by the properties of dependent sets).

		Now for $j = 1 \ldots q - p$, define the set
		$$ S_2 = \{ \mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_{p}, \mathbf{u}_{p+1}, \mathbf{v}_{p+1}, \ldots, \mathbf{v}_{p+j} \} . $$
		Let $T_k$ be the first of these sets that is linearly dependent. Then from our previous results we have
			\begin{align*}
				\mathbf{v}_{p+k} &\in \operatorname{span} (T_k \setminus \{ \mathbf{v}_{p+k} \}) \\
				\implies \mathbf{v}_{p+k} &\in \operatorname{span} (S_2 \setminus \{ \mathbf{v}_{p+k} \}) \\
				\\
				\implies \operatorname{span} (S_2 \setminus \{ \mathbf{v}_{p+k} \}) &= \operatorname{span} (S_2) \\
				&= V .
			\end{align*}
		Now take $S' = S_2 \setminus \{ \mathbf{v}_{p+k} \}$. Then we have shown that $S'$ spans $V$. Also, $T \subseteq S'$.

		Moreoever, $S'$ has one element fewer than $S_2$, and as $S_2$ has one element more than $S_1$, we have that $\lvert S' \rvert = \lvert S_1 \rvert = q$. This completes the proof of the inductive step.

		The result follows by induction.
	\end{proof}
\end{theorem}

\subsubsection{Coordinates}

\begin{definition}{\cite{math2601_notes}}{}
	Suppose $V$ is a vector space of dimension $n$ over $\mathbb{F}$ and suppose $\mathcal{B} = \{ \mathbf{v}_1, \ldots, \mathbf{v}_n \}$ is an ordered basis for $V$ over $\mathbb{F}$.

	If $\mathbf{v} \in V$ then $\mathbf{v} = \sum_{i = 1}^{n} \alpha_i \mathbf{v}_i$, with $\alpha_i$ unique, by theorem \ref{th:basis_written_uniquely}.

	We call
	$$ \boldsymbol{\alpha} = 
		\begin{pmatrix}
			\alpha_1 \\
			\vdots \\
			\alpha_n \\
		\end{pmatrix}
	$$
	the \textbf{coordinate vector} of $\mathbf{v}$ with respect to $\mathcal{B}$, and refer to the $\alpha_i$ as the \textbf{coordinates} of $\mathbf{v}$.
\end{definition}

A useful notation is
$$ \boldsymbol{\alpha} = [\mathbf{v}]_\mathcal{B} \quad
	\text{if} \quad \mathbf{v} = \sum_{i = 1}^{n} \alpha_i \mathbf{v}_i . $$

\subsubsection{Sums and Direct Sums}

\begin{definition}{Sum of subspaces \cite{math2601_notes}}{sum_subspaces}
	The \textbf{sum} $S + T$ of two subspaces is defined as
	$$ S + T = \{ \mathbf{a} + \mathbf{b} : \mathbf{a} \in S, \mathbf{b} \in T \} . $$
	If $S \cap T = \{ \mathbf{0} \}$, we call the sum a \textbf{direct sum} and denote it as $S \oplus T$.
\end{definition}

\begin{theorem}{\cite{math2601_notes}}{}
	Suppose $S$ and $T$ are finite dimensional subspaces of vector space $V$. Then
	$$ \dim (S) + \dim (T) = \dim (S + T) + \dim (S \cap T) . $$
\end{theorem}

\begin{corollary}{\cite{math2601_notes}}{}
	For a direct sum of finite dimensional spaces,
	$$ \dim (S) + \dim (T) = \dim (S \oplus T) . $$
\end{corollary}

\begin{lemma}{\cite{math2601_notes}}{}
	Let $V$ be a finite dimensional vector space and $X \leq V$. Then there is a subspace $Y$ for which $V = X \oplus Y$.
\end{lemma}

\begin{definition}{External direct sum \cite{math2601_notes}}{external_direct_sum}
	Let $X$ and $Y$ be two vector									 spaces over the same field $\mathbb{F}$. The Cartesian product $X \times Y$ can be made into a vector space over $\mathbb{F}$ with the obvious definitions
	$$
		(\mathbf{x}_1, \mathbf{y}_1) + (\mathbf{x}_2, \mathbf{y}_2)
		= (\mathbf{x}_1 + \mathbf{x}_2, \mathbf{y}_1 + \mathbf{y}_2)
	$$
	and
	$$ \lambda (\mathbf{x}_1, \mathbf{y}_1) = (\lambda \mathbf{x}_1, \lambda \mathbf{y}_1) . $$
	With this structure we call the Cartesian product the \textbf{(external) direct sum} of $X$ and $Y$ --- $X \oplus Y$.
\end{definition}