\section{Statistics}

\subsection{Probability}

\subsubsection{Probability spaces}

\begin{definition}{Probability space \cite{math2901_notes}}{probability_space}
In probability theory, a \textbf{probability space} or a \textbf{probability triple} $(\Omega, \mathcal{F}, P)$ is a mathematical construct that provides a formal model of a random process or ``experiment''.

A probability space consists of three elements:
\begin{enumerate}
	\item A \textbf{sample space} $\Omega$, which is the set of all possible outcomes.
	\item An \textbf{event space}, which is a set of events $\mathcal{F}$, an event being a set of outcomes in the sample space.
	\item A \textbf{probability function}, which assigns each event in the event space a probability, which is a number between 0 and 1.
\end{enumerate}
\end{definition}

The minimal assumption that we impose on $\mathcal{F}$ is that it should be an object called a $\sigma$-algebra.

\begin{definition}{$\sigma$-algebra \cite{wikipedia_sigma_algebra}}{sigma-algebra}
Let $X$ be some set, and let $\mathcal{P}(X)$ represent its power set. Then a subset $\Sigma \subseteq \mathcal{P}(X)$ is called a $\bm{\sigma}$\textbf{-algebra} if it satisfies the following three properties:

\begin{enumerate}
	\item $X$ is in $\Sigma$, and $X$ is considered to be the universal set in the following context;
	\item $\Sigma$ is closed under complementation: if $A$ is in $\Sigma$, then so is its complement $X \setminus A$;
	\item $\Sigma$ is closed under countable unions: if $A_1, A_2, A_3, \ldots$ are in $\Sigma$, then so is $A = A_1 \cup A_2 \cup A_3 \cup \ldots$
\end{enumerate}
\end{definition}

Given a sample space $(\Sigma, \mathcal{F})$, a probability function $\mathbb{P}$ can be defined in the following way: to every event $A \in \mathcal{F}$ we assign a number $\mathbb{P} (A)$, the \textbf{probability that $A$ occurs}. The function $\mathbb{P}$ must satisfy the axioms

\begin{enumerate}
	\item $\mathbb{P} (A) \geq 0$ for each $A \subset \Omega$;
	\item $\mathbb{P} (\Omega) = 1$;
	\item if $A_1, A_2, \ldots$ are mutually exclusive (disjoint), i.e.
	$$ A_i \cap A_j = \emptyset \text{ for all } i, j \text{ with } i \not = j  ,$$
	then
	$$ \mathbb{P} \left( \bigcup_{i = 1}^{\infty} A_i \right) = \sum_{i = 1}^{\infty} \mathbb{P} (A_i) . $$
\end{enumerate}

\begin{lemma}{\cite{math2901_notes}}{}
\begin{enumerate}
	\item If $A_1, A_2, \ldots, A_k$ are mutually exclusive, then
	$$ \mathbb{P} \left( \bigcup_{i = 1}^{k} A_i \right) = \sum_{i = 1}^{k} \mathbb{P} (A_i) ; $$
	\item $\mathbb{P} (\emptyset) = 0$;
	\item for any $A \subseteq \Omega$, it holds that $0 \leq \mathbb{P} (A) \leq 1$ and $\mathbb{P} (\bar A) = 1 - \mathbb{P} (A)$;
	\item if $B \subset A$, then $\mathbb{P} (B) \leq \mathbb{P} (A)$.
\end{enumerate}
\end{lemma}

\subsubsection{Monotonic sequences of events}

\begin{theorem}{Continuity properties \cite{math2901_notes}}{continuity_properties}
If $A_1, A_2, \ldots$ is an increasing sequence of events, i.e., $A_1 \subset A_2 \subset \dots$, then
$$ \lim_{n \to \infty} \mathbb{P} (A_n) = \mathbb{P} \left( \bigcup_{n = 1}^{\infty} \right) . $$
We say that $\mathbb{P}$ is \textbf{continuous from below}.

If $A_1, A_2, \ldots$ is a decreasing sequence of events, i.e., $A_1 \supseteq A_2 \supseteq \dots$, then
$$ \lim_{n \to \infty} \mathbb{P} (A_n) = \mathbb{P} \left( \bigcap_{n = 1}^{\infty} \right) . $$
We say that $\mathbb{P}$ is \textbf{continuous from above}.
\end{theorem}

\subsubsection{Conditional probability}

\begin{definition}{Conditional probability \cite{math2901_notes}}{conditional_probability}
The \textbf{conditional probability} that an event $A$ occurs, given that an event $B$ has occurred, is
$$ \mathbb{P} (A | B) = \dfrac{\mathbb{P} (A \cap B)}{\mathbb{P} (B)} \text{ if } \mathbb{P} (B) \not = 0 . $$
\end{definition}

\begin{lemma}{\cite{math2901_notes}}{conditional_probability}
$$ \mathbb{P} (A|B) = \mathbb{P} (A) \iff \mathbb{P} (B|A) = \mathbb{P} (B) $$
\end{lemma}

\subsubsection{Independent events}

\begin{definition}{Independent events \cite{math2901_notes}}{independent_events}
For a countable sequence of events $\{A_i\}$, the events are  
\begin{itemize}
	\item \textbf{pairwise independent} if
$$ \mathbb{P} (A_i \cap A_j) = \mathbb{P} (A_i) \mathbb{P} (A_j) \text{ for all } i \not = j ; $$

	\item \textbf{(mutually) independent} if for any sub-collection $A_{i_1}, \ldots, A_{i_n}$ we have
$$ \mathbb{P} \left( \bigcap_{j = 1}^n A_{i_j} \right) = \prod_{j = 1}^n \mathbb{P} (A_{i_j}) . $$
\end{itemize}
\end{definition}

Note that for any two events $A$ and $B$, $\mathbb{P} (A \cap B) = \mathbb{P} (A|B) \mathbb{P} (B)$, so $A$ and $B$ are independent if and only if the equalities in lemma \ref{lem:conditional_probability} are true.

Also note that mutual independence implies pairwise independence (but not vice versa).

\subsubsection{Probability laws}

\begin{theorem}{Multiplicative law}{multiplicative}
For events $A_1, A_2$,
$$ \mathbb{P} (A_1 \cap A_2) = \mathbb{P} (A_2 \cap A_1) = \mathbb{P} (A_2 \vert A_1) \mathbb{P} (A_1) . $$

For events $A_1, A_2, A_3$,
\begin{align*}
\mathbb{P} (A_1 \cap A_2 \cap A_3)
&= \mathbb{P} (A_3 \cap A_2 \cap A_1) \\
&= \mathbb{P} (A_3 \vert A_2 \cap A_1) \mathbb{P} (A_2 \cap A_1) \\
&= \mathbb{P} (A_3 \vert A_1 \cap A_2) \mathbb{P} (A_2 \vert A_1) \mathbb{P} (A_1) .
\end{align*}

(The same pattern applies to higher numbers of events.)
\end{theorem}

\begin{theorem}{Additive Law}{additive}
For events $A$ and $B$,
$$ \mathbb{P} (A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B) . $$
\end{theorem}

\begin{theorem}{Total probability}{total_probability}
Suppose $A_1, A_2, \ldots, A_k$ are mutually exclusive ---
$$ A_i \cap A_j = \emptyset \quad \text{for all } i \not = j $$
--- and \textbf{exhaustive} ---
$$ \bigcup_{i = 1}^k A_i = \Omega = \text{sample space} $$
--- that is, $A_1, \ldots, A_k$ form a \textbf{partition} of $\Omega$. Then, for any event $B$,
$$ \mathbb{P}(B) = \sum_{i=1}^k \mathbb{P}(B \vert A_i) \mathbb{P}(A_i) . $$
\end{theorem}

\begin{theorem}{Bayes' theorem}{bayes}
For a partition $A_1, A_2, \ldots, A_k$ and an event $B$,
$$
\mathbb{P}(A_j \vert B) 
= \dfrac{\mathbb{P}(B \vert A_j) \mathbb{P}(A_j)}{\mathbb{P}(B)} \\
= \dfrac{\mathbb{P}(B \vert A_j) \mathbb{P}(A_j)}{\sum_{i=1}^k \mathbb{P}(B|A_i) \mathbb{P}(A_i)}
$$
\end{theorem}

\subsection{Statistics}

\begin{definition}{Random variable \cite{wikipedia_random_variable}}{random_variable}
	A \textbf{random variable} is a measurable function $X : \Omega \to E$ from a set of possible outcomes $\Omega$ to a measurable space $E$. The technical axiomatic definition requires $\Omega$ to be a sample space of a probability triple $(\Omega, \mathcal{F}, P)$.

	The probability that $X$ takes on a vaue in a measurable set $S \subseteq E$ is written as
	$$ P(X \in S) = P(\{ \omega \in \Omega \vert X(\omega) \in S \}) . $$
\end{definition}

When the image (or range) of $X$ is countable, the random variable is called a \textbf{discrete random variable} and its distribution is a discrete probability distribution, i.e. can be described by a probability mass function that assigns a probability to each value in the image of $X$. If the image is uncountably infinite then $X$ is called a \textbf{continuous random variable}. In the special case that it is absolutely continuous, its distribution can be described by a probability density function, which assigns probabilities to intervals; in particular, each individual point must necessarily have probability zero for an absolutely continuous random variable. Not all continuous random variables are absolutely continuous, for example a mixture distribution. Such random variables cannot be described by a probability density or a probability mass function.

\begin{definition}{Cumulative distribution function \cite{wikipedia_cdf}}{cdf}
	The \textbf{cumulative distribution function} of a real-valued random variable $X$ is the function given by
	$$ F_X(x) = P(X \leq x) , $$
	where the right-hand side represents the probability that the random variable $X$ takes on a value less than or equal to $x$.
	
	The probability that $X$ lies in the semi-closed interval $(a, b]$, where $a < b$, is therefore
	$$ P(a < X \leq b) = F_X(b) - F_X(a) . $$
\end{definition}

Every cumulative distribution function $F_X$ is non-decreasing and right-continuous, which makes it a càdlàg function. Furthermore,
$$ \lim_{x \to -\infty} F_X(x) = 0, \quad \lim_{x \to +\infty} F_X(x) = 1 . $$

Also note that
$$ P(X = x) = F_X(x) - F_X(x-) , $$
where $F_X(x-)$ is shorthand notation for $\lim_{n \to \infty} F_X(x - 1/n)$. That is, the probability of $X = x$ is the size of the jump/change of the cumulative distribution function at the point $x$.

\begin{definition}{Probability mass function \cite{wikipedia_probability_mass_function}}{probability_mass_function}
	Probability mass function is the probability distribution of a discrete random variable, and provides the possible values and their associated probabilities. It is the function $p : \mathbb{R} \to [0, 1]$ defined by
	$$ p_X(x_i) = P(X = x_i) $$
	for $-\infty < x < \infty$, where $P$ is a probability measure; $p_X(x)$ can also be simplified as $p(x)$.

	The probabilities associated with each possible values must be positive and sum up to 1. For all other values, the probabilities need to be 0:
	\begin{align*}
		\sum p_X(x_i) &= 1 ; \\
		p(x_i) &> 0 ; \\
		p(x) &= 0 \quad \text{for all other } x .
	\end{align*}
\end{definition}

Thinking of probability as mass helps to avoid mistakes, since the physical mass is conserved as is the total probability for all hypothetical outcomes $x$.

\begin{definition}{Probability density function \cite{wikipedia_probability_density_function}}{probability_density_function}
	A random variable $X$ with values in a measurable space $(\mathcal{X}, \mathcal{A})$ (usually $\mathbb{R}^n$ with the Borel sets as a measurable subsets) has as probability distribution the measure $X_* P$ on $(\mathcal{X}, \mathcal{A})$: the \textbf{density} of $X$ with respect to a reference measure $\mu$ on $(\mathcal{X}, \mathcal{A})$ is the Radon-Nikodym derivative
	$$ f = \dfrac{dX_* P}{d \mu} . $$
	That is, $f$ is any measurable function with the property that
	$$ P(X \in A) = \int_{X^{-1}A} dP = \int_A f d\mu $$
	for any measurable set $A \in \mathcal{A}$.
\end{definition}

For a function $f : \mathbb{R} \to \mathbb{R}$ to be a valid density function, the function $f$ must satisfy the following properties \cite{math2901_notes}:
\begin{enumerate}
	\item for all $x \in \mathbb{R}$, $f(x) \geq 0$;
	\item $\int_{-\infty}^{+\infty} f(x) dx = 1$.
\end{enumerate}

\begin{definition}{Expected value \cite{wikipedia_expected_value}}{expected_value}
	\begin{itemize}

		\item \textsc{Discrete case.} Let $X$ be a random variable with a finite number of finite outcomes $x_1, x_2, \ldots, x_k$ occurring with probabilities $p_1, p_2, \ldots, p_k$, repsectively. The \textbf{expectation} of $X$ is defined as
		$$ E(X) = \sum_{i=1}^k x_i p_i. $$
		Since the sum of all probabilites $p_i$ is 1, the expected value is the weighted average of the $x_i$ values, with the $p_i$ values being the weights.
	
		If all outcomes $x_{i}$ are equiprobable (that is, $p_{1} = p_{2} = \cdots = p_{k}$), then the weighted average turns into the simple average. If the outcomes $x_{i}$ are not equiprobable, then the simple average must be replaced with the weighted average, which takes into account the fact that some outcomes are more likely than the others.
		
		\item \textsc{Absolutely continuous case.} If $X$ is a random variable with a probability density function of $f$, then the expected value is deinfed as the Lebesgue integral
		$$ E(X) = \int_{\mathbb{R}} x f(x) dx , $$
		where the values on both sides are well defined or not well defined simultaneously.

	\end{itemize}
\end{definition}

In classical mechanics, the center of mass is an analogous concept to expectation. For example, suppose $X$ is a discrete random variable with values $x_i$ and corresponding probabilities $p_i$. Now consider a weightless rod on which are placed weights, at locations $x_i$ along the rod and having masses $p_i$ (whose sum is 1). The point at which the rod balances is $E(X)$ \cite{wikipedia_expected_value}.

\begin{lemma}{Expectation of transformed variable \cite{math2901_notes}}{expectation_transformed_variable}
	Suppose $g : \mathbb{R} \to \mathbb{R}$. Then the expectation of the transformed random variable $g(X)$ is
	$$
	E(g(X)) = 
		\begin{cases}
			\int_{\mathbb{R}} g(x) f_X(x) dx \quad & \text{\textsc{Continuous case.}} \\
			\sum_x g(x) P(X = x) \quad & \text{\textsc{Discrete case.}}
		\end{cases}
	$$
\end{lemma}

Usually, one is interested in computing $E(X^r)$ for $r \in \mathbb{N}$, which is called the $r$-th moment of $X$. \cite{math2901_notes}

In mathematics, a \textbf{moment} is a specific quantitative measure of the shape of a function.

The concept is used in both mechanics and statistics. If the function represents physical density, then the zeroth moment is the total mass, the first moment divided by the total mass is the center of mass, and the second moment is the rotational inertia. If the function is a probability distribution, then the zeroth moment is the total probability (i.e. one), the first moment is the expected value, the second central moment is the variance, the third standardized moment is the skewness, and the fourth standardized moment is the kurtosis. The mathematical concept is closely related to the concept of moment in physics. \cite{wikipedia_moment_mathematics}

\begin{lemma}{Linearity of expectation \cite{wikipedia_expected_value}}{lineary_expectation}
	The expected value operator (or expectation operator) $E(\cdot)$ is linear in the sense that, for any random variables $X$ and $Y$, and a constant $a$,
	\begin{align*}
		E(X + Y) &= E(X) + E(Y); \\
		E(aX) &= a E(X),
	\end{align*}
	whenever the right-hand side is well-defined.
\end{lemma}

\begin{definition}{Variance \cite{wikipedia_variance}}{variance}
	The variance of a random variable $X$ is the expected value of the squared deviation from the mean of $X$, $\mu = E(X)$:
	$$ \sigma^2 = \operatorname{Var}(X) = E((X - \mu)^2) . $$
\end{definition}

Intuitively, the variance measures on average how much the random variable deviates from its expectation/mean. \cite{math2901_notes}

\begin{lemma}{Properties of variance \cite{math2901_notes}}{properties_variance}
	Given a random variable $X$, for any constants $a, b \in \mathbb{R}$,
	\begin{itemize}
		\item $ \operatorname{Var}(X) = E(X^2) - (E(X))^2 $;
		\item $ \operatorname{Var}(aX) = a^2 \operatorname{Var}(X) $;
		\item $ \operatorname{Var}(X + b) = \operatorname{Var}(X) $;
		\item $ \operatorname{Var}(b) = 0 $.
	\end{itemize}
\end{lemma}

\begin{definition}{Standard deviation \cite{wikipedia_standard_deviation}}{standard_deviation}
	Let $X$ be a random variable with mean value $\mu$:
	$$ E(X) = \mu . $$
	Here the operator $E$ denotes the average or expected value of $X$. Then the standard deviation of $X$ is the quantity
	\begin{align*}
		\sigma &= \sqrt{\operatorname{Var}(X)} \\
		&= \sqrt{E((X - \mu)^2)} .
	\end{align*}
\end{definition}

\begin{definition}{Moment-generating function \cite{wikipedia_moment_generating_function}}{moment_generating_function}
	The moment-generating function of a random variable $X$ is
	$$ M_X(t) = E(e^{tX}) , $$
	wherever this expecation exists. In other words, the moment-generating function is the expectation of the random variable $e^{tX}$.
\end{definition}

The moment-generating function of $X$ exists if there exists $h > 0$ such that the $M_X(t)$ is finite for $x \in [-h, h]$.

\begin{lemma}{Calculations of moments \cite{wikipedia_moment_generating_function}}{calculation_moments}
	The moment-generating function is so called because if it exists on an open interval around $t = 0$, then it is the exponential generating function of the moments of the probability distribution:
	\begin{align*}
		m_{n} &= E\left(X^{n}\right) \\
		&= M_{X}^{(n)}(0) \\
		&= \left.{\frac {d^{n}M_{X}}{dt^{n}}}\right|_{t=0} .
	\end{align*}
	That is, with $n$ being a nonnegative integer, the $n$th moment about 0 is the $n$th derivative of the moment generating function, evaluated at $t = 0$.	
\end{lemma}