\section{Statistics}

\subsection{Probability}

\subsubsection{Probability spaces}

\begin{definition}{Probability space \cite{math2901_notes}}{probability_space}
In probability theory, a \textbf{probability space} or a \textbf{probability triple} $(\Omega, \mathcal{F}, P)$ is a mathematical construct that provides a formal model of a random process or ``experiment''.

A probability space consists of three elements:
\begin{enumerate}
	\item A \textbf{sample space} $\Omega$, which is the set of all possible outcomes.
	\item An \textbf{event space}, which is a set of events $\mathcal{F}$, an event being a set of outcomes in the sample space.
	\item A \textbf{probability function}, which assigns each event in the event space a probability, which is a number between 0 and 1.
\end{enumerate}
\end{definition}

The minimal assumption that we impose on $\mathcal{F}$ is that it should be an object called a $\sigma$-algebra.

\begin{definition}{$\sigma$-algebra \cite{wikipedia_sigma_algebra}}{sigma-algebra}
Let $X$ be some set, and let $\mathcal{P}(X)$ represent its power set. Then a subset $\Sigma \subseteq \mathcal{P}(X)$ is called a $\bm{\sigma}$\textbf{-algebra} if it satisfies the following three properties:

\begin{enumerate}
	\item $X$ is in $\Sigma$, and $X$ is considered to be the universal set in the following context;
	\item $\Sigma$ is closed under complementation: if $A$ is in $\Sigma$, then so is its complement $X \setminus A$;
	\item $\Sigma$ is closed under countable unions: if $A_1, A_2, A_3, \ldots$ are in $\Sigma$, then so is $A = A_1 \cup A_2 \cup A_3 \cup \ldots$
\end{enumerate}
\end{definition}

Given a sample space $(\Sigma, \mathcal{F})$, a probability function $\mathbb{P}$ can be defined in the following way: to every event $A \in \mathcal{F}$ we assign a number $\mathbb{P} (A)$, the \textbf{probability that $A$ occurs}. The function $\mathbb{P}$ must satisfy the axioms

\begin{enumerate}
	\item $\mathbb{P} (A) \geq 0$ for each $A \subset \Omega$;
	\item $\mathbb{P} (\Omega) = 1$;
	\item if $A_1, A_2, \ldots$ are mutually exclusive (disjoint), i.e.
	$$ A_i \cap A_j = \emptyset \text{ for all } i, j \text{ with } i \not = j  ,$$
	then
	$$ \mathbb{P} \left( \bigcup_{i = 1}^{\infty} A_i \right) = \sum_{i = 1}^{\infty} \mathbb{P} (A_i) . $$
\end{enumerate}

\begin{lemma}{\cite{math2901_notes}}{}
\begin{enumerate}
	\item If $A_1, A_2, \ldots, A_k$ are mutually exclusive, then
	$$ \mathbb{P} \left( \bigcup_{i = 1}^{k} A_i \right) = \sum_{i = 1}^{k} \mathbb{P} (A_i) ; $$
	\item $\mathbb{P} (\emptyset) = 0$;
	\item for any $A \subseteq \Omega$, it holds that $0 \leq \mathbb{P} (A) \leq 1$ and $\mathbb{P} (\bar A) = 1 - \mathbb{P} (A)$;
	\item if $B \subset A$, then $\mathbb{P} (B) \leq \mathbb{P} (A)$.
\end{enumerate}
\end{lemma}

\subsubsection{Monotonic sequences of events}

\begin{theorem}{Continuity properties \cite{math2901_notes}}{continuity_properties}
If $A_1, A_2, \ldots$ is an increasing sequence of events, i.e., $A_1 \subset A_2 \subset \dots$, then
$$ \lim_{n \to \infty} \mathbb{P} (A_n) = \mathbb{P} \left( \bigcup_{n = 1}^{\infty} \right) . $$
We say that $\mathbb{P}$ is \textbf{continuous from below}.

If $A_1, A_2, \ldots$ is a decreasing sequence of events, i.e., $A_1 \supseteq A_2 \supseteq \dots$, then
$$ \lim_{n \to \infty} \mathbb{P} (A_n) = \mathbb{P} \left( \bigcap_{n = 1}^{\infty} \right) . $$
We say that $\mathbb{P}$ is \textbf{continuous from above}.
\end{theorem}

\subsubsection{Conditional probability}

\begin{definition}{Conditional probability \cite{math2901_notes}}{conditional_probability}
The \textbf{conditional probability} that an event $A$ occurs, given that an event $B$ has occurred, is
$$ \mathbb{P} (A | B) = \dfrac{\mathbb{P} (A \cap B)}{\mathbb{P} (B)} \text{ if } \mathbb{P} (B) \not = 0 . $$
\end{definition}

\begin{lemma}{\cite{math2901_notes}}{conditional_probability}
$$ \mathbb{P} (A|B) = \mathbb{P} (A) \iff \mathbb{P} (B|A) = \mathbb{P} (B) $$
\end{lemma}

\subsubsection{Independent events}

\begin{definition}{Independent events \cite{math2901_notes}}{independent_events}
For a countable sequence of events $\{A_i\}$, the events are  
\begin{itemize}
	\item \textbf{pairwise independent} if
$$ \mathbb{P} (A_i \cap A_j) = \mathbb{P} (A_i) \mathbb{P} (A_j) \text{ for all } i \not = j ; $$

	\item \textbf{(mutually) independent} if for any sub-collection $A_{i_1}, \ldots, A_{i_n}$ we have
$$ \mathbb{P} \left( \bigcap_{j = 1}^n A_{i_j} \right) = \prod_{j = 1}^n \mathbb{P} (A_{i_j}) . $$
\end{itemize}
\end{definition}

Note that for any two events $A$ and $B$, $\mathbb{P} (A \cap B) = \mathbb{P} (A|B) \mathbb{P} (B)$, so $A$ and $B$ are independent if and only if the equalities in lemma \ref{lem:conditional_probability} are true.

Also note that mutual independence implies pairwise independence (but not vice versa).

\subsubsection{Probability laws}

\begin{theorem}{Multiplicative law}{multiplicative}
For events $A_1, A_2$,
$$ \mathbb{P} (A_1 \cap A_2) = \mathbb{P} (A_2 \cap A_1) = \mathbb{P} (A_2 \vert A_1) \mathbb{P} (A_1) . $$

For events $A_1, A_2, A_3$,
\begin{align*}
\mathbb{P} (A_1 \cap A_2 \cap A_3)
&= \mathbb{P} (A_3 \cap A_2 \cap A_1) \\
&= \mathbb{P} (A_3 \vert A_2 \cap A_1) \mathbb{P} (A_2 \cap A_1) \\
&= \mathbb{P} (A_3 \vert A_1 \cap A_2) \mathbb{P} (A_2 \vert A_1) \mathbb{P} (A_1) .
\end{align*}

(The same pattern applies to higher numbers of events.)
\end{theorem}

\begin{theorem}{Additive Law}{additive}
For events $A$ and $B$,
$$ \mathbb{P} (A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B) . $$
\end{theorem}

\begin{theorem}{Total probability}{total_probability}
Suppose $A_1, A_2, \ldots, A_k$ are mutually exclusive ---
$$ A_i \cap A_j = \emptyset \quad \text{for all } i \not = j $$
--- and \textbf{exhaustive} ---
$$ \bigcup_{i = 1}^k A_i = \Omega = \text{sample space} $$
--- that is, $A_1, \ldots, A_k$ form a \textbf{partition} of $\Omega$. Then, for any event $B$,
$$ \mathbb{P}(B) = \sum_{i=1}^k \mathbb{P}(B \vert A_i) \mathbb{P}(A_i) . $$
\end{theorem}

\begin{theorem}{Bayes' theorem}{bayes}
For a partition $A_1, A_2, \ldots, A_k$ and an event $B$,
$$
\mathbb{P}(A_j \vert B) 
= \dfrac{\mathbb{P}(B \vert A_j) \mathbb{P}(A_j)}{\mathbb{P}(B)} \\
= \dfrac{\mathbb{P}(B \vert A_j) \mathbb{P}(A_j)}{\sum_{i=1}^k \mathbb{P}(B|A_i) \mathbb{P}(A_i)}
$$
\end{theorem}

\subsection{Probability functions, expectation, variance}

\begin{definition}{Random variable \cite{wikipedia_random_variable}}{random_variable}
	A \textbf{random variable} is a measurable function $X : \Omega \to E$ from a set of possible outcomes $\Omega$ to a measurable space $E$. The technical axiomatic definition requires $\Omega$ to be a sample space of a probability triple $(\Omega, \mathcal{F}, P)$.

	The probability that $X$ takes on a vaue in a measurable set $S \subseteq E$ is written as
	$$ P(X \in S) = P(\{ \omega \in \Omega \vert X(\omega) \in S \}) . $$
\end{definition}

When the image (or range) of $X$ is countable, the random variable is called a \textbf{discrete random variable} and its distribution is a discrete probability distribution, i.e. can be described by a probability mass function that assigns a probability to each value in the image of $X$. If the image is uncountably infinite then $X$ is called a \textbf{continuous random variable}. In the special case that it is absolutely continuous, its distribution can be described by a probability density function, which assigns probabilities to intervals; in particular, each individual point must necessarily have probability zero for an absolutely continuous random variable. Not all continuous random variables are absolutely continuous, for example a mixture distribution. Such random variables cannot be described by a probability density or a probability mass function.

\begin{definition}{Cumulative distribution function \cite{wikipedia_cdf}}{cdf}
	The \textbf{cumulative distribution function} of a real-valued random variable $X$ is the function given by
	$$ F_X(x) = P(X \leq x) , $$
	where the right-hand side represents the probability that the random variable $X$ takes on a value less than or equal to $x$.
	
	The probability that $X$ lies in the semi-closed interval $(a, b]$, where $a < b$, is therefore
	$$ P(a < X \leq b) = F_X(b) - F_X(a) . $$
\end{definition}

Every cumulative distribution function $F_X$ is non-decreasing and right-continuous, which makes it a càdlàg function. Furthermore,
$$ \lim_{x \to -\infty} F_X(x) = 0, \quad \lim_{x \to +\infty} F_X(x) = 1 . $$

Also note that
$$ P(X = x) = F_X(x) - F_X(x-) , $$
where $F_X(x-)$ is shorthand notation for $\lim_{n \to \infty} F_X(x - 1/n)$. That is, the probability of $X = x$ is the size of the jump/change of the cumulative distribution function at the point $x$.

\begin{definition}{Probability mass function \cite{wikipedia_probability_mass_function}}{probability_mass_function}
	Probability mass function is the probability distribution of a discrete random variable, and provides the possible values and their associated probabilities. It is the function $p : \mathbb{R} \to [0, 1]$ defined by
	$$ p_X(x_i) = P(X = x_i) $$
	for $-\infty < x < \infty$, where $P$ is a probability measure; $p_X(x)$ can also be simplified as $p(x)$.

	The probabilities associated with each possible values must be positive and sum up to 1. For all other values, the probabilities need to be 0:
	\begin{align*}
		\sum p_X(x_i) &= 1 ; \\
		p(x_i) &> 0 ; \\
		p(x) &= 0 \quad \text{for all other } x .
	\end{align*}
\end{definition}

Thinking of probability as mass helps to avoid mistakes, since the physical mass is conserved as is the total probability for all hypothetical outcomes $x$.

\begin{definition}{Probability density function \cite{wikipedia_probability_density_function}}{probability_density_function}
	A random variable $X$ with values in a measurable space $(\mathcal{X}, \mathcal{A})$ (usually $\mathbb{R}^n$ with the Borel sets as a measurable subsets) has as probability distribution the measure $X_* P$ on $(\mathcal{X}, \mathcal{A})$: the \textbf{density} of $X$ with respect to a reference measure $\mu$ on $(\mathcal{X}, \mathcal{A})$ is the Radon-Nikodym derivative
	$$ f = \dfrac{dX_* P}{d \mu} . $$
	That is, $f$ is any measurable function with the property that
	$$ P(X \in A) = \int_{X^{-1}A} dP = \int_A f d\mu $$
	for any measurable set $A \in \mathcal{A}$.
\end{definition}

For a function $f : \mathbb{R} \to \mathbb{R}$ to be a valid density function, the function $f$ must satisfy the following properties \cite{math2901_notes}:
\begin{enumerate}
	\item for all $x \in \mathbb{R}$, $f(x) \geq 0$;
	\item $\int_{-\infty}^{+\infty} f(x) dx = 1$.
\end{enumerate}

\begin{definition}{Expected value \cite{wikipedia_expected_value}}{expected_value}
	\begin{itemize}

		\item \textsc{Discrete case.} Let $X$ be a random variable with a finite number of finite outcomes $x_1, x_2, \ldots, x_k$ occurring with probabilities $p_1, p_2, \ldots, p_k$, repsectively. The \textbf{expectation} of $X$ is defined as
		$$ E(X) = \sum_{i=1}^k x_i p_i. $$
		Since the sum of all probabilites $p_i$ is 1, the expected value is the weighted average of the $x_i$ values, with the $p_i$ values being the weights.
	
		If all outcomes $x_{i}$ are equiprobable (that is, $p_{1} = p_{2} = \cdots = p_{k}$), then the weighted average turns into the simple average. If the outcomes $x_{i}$ are not equiprobable, then the simple average must be replaced with the weighted average, which takes into account the fact that some outcomes are more likely than the others.
		
		\item \textsc{Absolutely continuous case.} If $X$ is a random variable with a probability density function of $f$, then the expected value is deinfed as the Lebesgue integral
		$$ E(X) = \int_{\mathbb{R}} x f(x) dx , $$
		where the values on both sides are well defined or not well defined simultaneously.

	\end{itemize}
\end{definition}

In classical mechanics, the center of mass is an analogous concept to expectation. For example, suppose $X$ is a discrete random variable with values $x_i$ and corresponding probabilities $p_i$. Now consider a weightless rod on which are placed weights, at locations $x_i$ along the rod and having masses $p_i$ (whose sum is 1). The point at which the rod balances is $E(X)$ \cite{wikipedia_expected_value}.

\begin{lemma}{Expectation of transformed variable \cite{math2901_notes}}{expectation_transformed_variable}
	Suppose $g : \mathbb{R} \to \mathbb{R}$. Then the expectation of the transformed random variable $g(X)$ is
	$$
	E(g(X)) = 
		\begin{cases}
			\int_{\mathbb{R}} g(x) f_X(x) dx \quad & \text{\textsc{Continuous case.}} \\
			\sum_x g(x) P(X = x) \quad & \text{\textsc{Discrete case.}}
		\end{cases}
	$$
\end{lemma}

Usually, one is interested in computing $E(X^r)$ for $r \in \mathbb{N}$, which is called the $r$-th moment of $X$. \cite{math2901_notes}

In mathematics, a \textbf{moment} is a specific quantitative measure of the shape of a function.

The concept is used in both mechanics and statistics. If the function represents physical density, then the zeroth moment is the total mass, the first moment divided by the total mass is the center of mass, and the second moment is the rotational inertia. If the function is a probability distribution, then the zeroth moment is the total probability (i.e. one), the first moment is the expected value, the second central moment is the variance, the third standardized moment is the skewness, and the fourth standardized moment is the kurtosis. The mathematical concept is closely related to the concept of moment in physics. \cite{wikipedia_moment_mathematics}

\begin{lemma}{Linearity of expectation \cite{wikipedia_expected_value}}{lineary_expectation}
	The expected value operator (or expectation operator) $E(\cdot)$ is linear in the sense that, for any random variables $X$ and $Y$, and a constant $a$,
	\begin{align*}
		E(X + Y) &= E(X) + E(Y); \\
		E(aX) &= a E(X),
	\end{align*}
	whenever the right-hand side is well-defined.
\end{lemma}

\begin{definition}{Variance \cite{wikipedia_variance}}{variance}
	The variance of a random variable $X$ is the expected value of the squared deviation from the mean of $X$, $\mu = E(X)$:
	$$ \sigma^2 = \operatorname{Var}(X) = E((X - \mu)^2) . $$
\end{definition}

Intuitively, the variance measures on average how much the random variable deviates from its expectation/mean. \cite{math2901_notes}

\begin{lemma}{Properties of variance \cite{math2901_notes}}{properties_variance}
	Given a random variable $X$, for any constants $a, b \in \mathbb{R}$,
	\begin{itemize}
		\item $ \operatorname{Var}(X) = E(X^2) - (E(X))^2 $;
		\item $ \operatorname{Var}(aX) = a^2 \operatorname{Var}(X) $;
		\item $ \operatorname{Var}(X + b) = \operatorname{Var}(X) $;
		\item $ \operatorname{Var}(b) = 0 $.
	\end{itemize}
\end{lemma}

\begin{definition}{Standard deviation \cite{wikipedia_standard_deviation}}{standard_deviation}
	Let $X$ be a random variable with mean value $\mu$:
	$$ E(X) = \mu . $$
	Here the operator $E$ denotes the average or expected value of $X$. Then the standard deviation of $X$ is the quantity
	\begin{align*}
		\sigma &= \sqrt{\operatorname{Var}(X)} \\
		&= \sqrt{E((X - \mu)^2)} .
	\end{align*}
\end{definition}

\begin{definition}{Moment-generating function \cite{wikipedia_moment_generating_function}}{moment_generating_function}
	The moment-generating function of a random variable $X$ is
	$$ M_X(t) = E(e^{tX}) , $$
	wherever this expecation exists. In other words, the moment-generating function is the expectation of the random variable $e^{tX}$.
\end{definition}

The moment-generating function of $X$ exists if there exists $h > 0$ such that the $M_X(t)$ is finite for $x \in [-h, h]$.

\begin{lemma}{Calculations of moments \cite{wikipedia_moment_generating_function}}{calculation_moments}
	The moment-generating function is so called because if it exists on an open interval around $t = 0$, then it is the exponential generating function of the moments of the probability distribution:
	\begin{align*}
		m_{n} &= E\left(X^{n}\right) \\
		&= M_{X}^{(n)}(0) \\
		&= \left.{\frac {d^{n}M_{X}}{dt^{n}}}\right|_{t=0} .
	\end{align*}
	That is, with $n$ being a nonnegative integer, the $n$th moment about 0 is the $n$th derivative of the moment generating function, evaluated at $t = 0$.	
\end{lemma}

\begin{theorem}{\cite{math2901_notes}}{}
	Let $X$ and $Y$ be two random variables such that their respective moment-generating functions $M_X(t)$ and $M_Y(t)$ exist.

	If both moment-generating functions are equal, then the cumulative distributive functions of $X$ and $Y$ are also equal, i.e.

	\begin{align*}
		M_X(t) &= M_Y(t) \\
		\implies F_X(x) &= F_Y(x) \quad \text{for all } x \in \mathbb{R}
	\end{align*}
\end{theorem}

The above theorem tells you that if the moment generating
function exists then it uniquely characterises the cumulative
distribution function of the random variable. \cite{math2901_notes}

Note that if the moment-generating function of a random variable exists then all moments can be computed; the converse is not true in general.

\begin{lemma}{Markov's inequality \cite{wikipedia_markovs_inequality}}{markovs_inequality}
	If $X$ is a nonnegative random variable and $a > 0$, then the probability that $X$ is at least $a$ is at most the expectation of $X$ divided by $a$:

	$$ \operatorname {P} (X\geq a)\leq {\frac {\operatorname {E} (X)}{a}} . $$
\end{lemma}

\begin{lemma}{Chebyshev's inequality \cite{wikipedia_chebyshevs_inequality}}{chebyshevs_inequality}
	Let $X$ (integrable) be a random variable with finite expected value $\mu$ and finite non-zero variance $\sigma^2$. Then for any real number $k > 0$,

	$$ P(\lvert X-\mu \rvert \geq k \sigma) \leq \frac {1}{k^{2}} . $$
\end{lemma}

\begin{definition}{Convex/concave function \cite{math2901_notes}}{convex_function}
	Let $X$ be a convex set in a real vector space and let $f : X \to \mathbb{R}$ be a function.
	\begin{itemize}
		\item $f$ is called \textbf{convex} if $\forall x_1, x_2 \in X, \forall t \in [0, 1]$:
		$$ f(t x_1 + (1 - t) x_2) \leq t f(x_1) + (1 - t) f(x_2) $$

		\item $f$ is \textbf{strictly convex} if $\forall x_1 \neq x_2 \in X, \forall t \in (0, 1)$:
		$$ f(t x_1 + (1 - t) x_2) < t f(x_1) + (1 - t) f(x_2) $$

		\item A function $f$ is said to be (strictly) concave if $-f$ is (strictly) convex.
	\end{itemize}	
\end{definition}

\begin{lemma}{Jensen's inequality \cite{wikipedia_jensens_inequality}}{jensens_inequality}
	If $X$ is a random variable and $\phi$ is a convex function, then
	$$ \phi(E(X)) \leq E(\phi(X)) . $$

	If $\phi$ is a concave function, then
	$$ \phi(E(X)) \geq E(\phi(X)) . $$
\end{lemma}

\subsection{Distributions}

\subsubsection{Common discrete distributions}

\begin{definition}{Bernoulli trial \cite{wikipedia_bernoulli_trial}}{bernoulli_trial}
	Independent repeated trials of an experiment with exactly two possible outcomes are called Bernoulli trials.
\end{definition}

\begin{definition}{Binomial distribution \cite{wikipedia_binomial_distribution}}{binomial_distribution}
	In general, if the random variable $X$ follows the binomial distribution with parameters $n \in \mathbb{N}$ and $p \in [0, 1]$, we write $X ~ B(n, p)$. The probability of getting exactly $k$ successes in $n$ independent Bernoulli trials is given by the probability mass function:
	\begin{align*}
		f(k,n,p) &= \Pr(k; n, p) \\
		&= \Pr(X = k) \\
		&= {\binom{n}{k}} p^{k} (1 - p)^{n - k}
	\end{align*}
	for $k = 0, 1, 2, ..., n$, where
	$$ {\binom{n}{k}} = \frac {n!}{k!(n-k)!} $$
	is the binomial coefficient, hence the name of the distribution. 
\end{definition}

The formula can be understood as follows. $k$ successes occur with probability $p^k$ and $n - k$ failures occur with probability $(1 - p)^{n - k}$. However, the $k$ successes can occur anywhere among the $n$ trials, and there are $\binom{n}{k}$ different ways of distributing $k$ successes in a sequence of $n$ trials. \cite{wikipedia_binomial_distribution}

\begin{definition}{Hypergeometric distribution \cite{wikipedia_hypergeometric_distribution}}{hypergeometric_distribution}
	The following conditions characterize the hypergeometric distribution:

	\begin{itemize}
		\item The result of each draw (the elements of the population being sampled) can be classified into one of two mutually exclusive categories (e.g. Pass/Fail or Employed/Unemployed).
		\item The probability of a success changes on each draw, as each draw decreases the population (sampling without replacement from a finite population).
	\end{itemize}	
	
	A random variable $X$ follows the hypergeometric distribution if its probability mass function (pmf) is given by
	$$ 
		p_X(k) = \Pr(X = k) = \frac{\binom{K}{k} \binom{N-K}{n-k}}{\binom{N}{n}},
	$$
	where
	\begin{itemize}
		\item $N$ is the population size,
		\item $K$ is the number of success states in the population,
		\item $n$ is the number of draws (i.e. quantity drawn in each trial),
		\item $k$ is the number of observed successes, and
		\item $a \choose b$ is a binomial coefficient.
	\end{itemize}
\end{definition}

\begin{itemize}
	\item The binomial distribution differs from the hypergeometric distribution in that it describes the probability of $k$ successes in $n$ draws \textit{with} replacement.
	\item The pmf is positive when $\max(0, n + K - N) \leq k \leq \min(K, n)$.
	\item A random variable distributed hypergeometrically with parameters $N, K$ and $n$ is written $X \sim \operatorname{Hypergeometric}(N, K, n)$ and has probability mass function $p_X(k)$ above.
\end{itemize}

\begin{definition}{Poisson distribution \cite{wikipedia_poisson_distribution}}{}
	The \textbf{Poisson distribution} is popular for modeling the number of times an event occurs in an interval of time or space.

	A discrete random variable $X$ is said to have a Poisson distribution with parameter $\lambda > 0$, if, for $k = 0, 1, 2, \ldots$, the probability mass function of $X$ is given by
	$$ f(k; \lambda) = \Pr(X = k)= \frac {\lambda^k e^{-\lambda}}{k!} , $$
	where (the positive real number) $\lambda = E(X) = \operatorname{Var}(X)$.
\end{definition}

\begin{definition}{Normal (or Gaussan) distribution \cite{}}{normal_distribution}
	A \textbf{normal} (or \textbf{Gaussian} or \textbf{Gauss} or \textbf{Laplace–Gauss}) \textbf{distribution} is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is
	$$
		f(x) = \frac{1}{\sigma {\sqrt{2 \pi}}} 
		e^{-{\frac{1}{2}} \left( {\frac{x-\mu}{\sigma}} \right)^2}
	$$
	The parameter $\mu$ is the mean or expectation of the distribution (and also its median and mode); and $\sigma$ is its standard deviation. The variance of the distribution is $\sigma ^{2}$. A random variable with a Gaussian distribution is said to be \textbf{normally distributed} and is called a \textbf{normal deviate}.
\end{definition}

The normal distribution is often referred to as $N(\mu, \sigma^2)$ or $\mathcal{N}(\mu, \sigma^2)$. Thus when a random variable $X$ is distributed normally with mean $\mu$ and variance $\sigma^2$, one may write
$$ X \sim \mathcal{N}(\mu, \sigma^2) . $$

\begin{lemma}{Linear transform \cite{math2901_notes}}{linear_transform}
	Let $X$ be a random variable with probability density function $f_X$, and let $Y = a + bX$. Then for $b > 0$ and $a \in \mathbb{R}$,
	$$ f_Y(x) = \frac{1}{b} f_X \left( \frac{x - a}{b} \right) . $$
\end{lemma}

\begin{lemma}{\cite{math2901_notes}}{}
	Suppose $X \sim \mathcal{N}(\mu, \sigma^2)$ and $a \in \mathbb{R}$ and $b > 0$. The random variable $Y = a + bX$ is also normally distributed with parameter $(a + b \mu, b^2 \sigma^2)$, i.e. $Y \sim \mathcal{N}(a + b \mu, b^2 \sigma^2)$.
\end{lemma}

\begin{itemize}
	\item Linear transforms of normal random variable are again	normal random variables.
	\item If a random variable is normally distributed, then it is completely
	characterized by the mean $\mu$ and the variance $\sigma^2$.	
\end{itemize}

\begin{corollary}{Standardising \cite{math2901_notes}}{standardising}
	Suppose $X \sim \mathcal{N}(\mu, \sigma^2)$. Then
	$$ Z = \frac{X - \mu}{\sigma} \sim \mathcal{N}(0, 1) . $$
\end{corollary}

The procedure of transforming a $\mathcal{N}(\mu, \sigma^2)$ random variable into a $\mathcal{N}(0, 1)$ random variable is called \textbf{standardising}. The reason for doing this is that
$$ F_X(y) = \int_0^y f_X(x) dx $$
is impossible to compute. The values of the cumulative distribution function of the $\mathcal{N}(0, 1)$ random variable is tabulated and must be looked up a table.

\begin{definition}{Exponential distribution \cite{wikipedia_exponential_distribution}}{exponential_distribution}
	The probability density function of an \textbf{exponential distribution} is
	$$
		f(x; \lambda) = 
			\begin{cases}
				\lambda e^{-\lambda x} \quad &x \geq 0, \\
				0 \quad &x < 0. 
			\end{cases}
	$$
	Here $\lambda > 0$ is the parameter of the distribution, often called the \textit{rate parameter}. The distribution is upposrted on the interval $[0, \infty)$. If a random variable $X$ has this distribution, we write $X \sim \operatorname{Exp}(\lambda)$.

	The exponential distribution is sometimes parametrised in terms of the scaled parameter $\beta = 1/\lambda$:
	$$
		f(x; \lambda) = 
			\begin{cases}
				\frac{1}{\beta} e^{-x/\beta} \quad &x \geq 0, \\
				0 \quad &x < 0. 
			\end{cases}
	$$
\end{definition}

\begin{definition}{Gamma distribution \cite{wikipedia_gamma_distribution}}{gamma_distribution}
	The \textbf{gamma distribution} can be parameterised in terms of a shape parameter $\alpha = k$ and an inverse scale parameter $\beta = 1/\theta$, called a rate parameter. A random variable $X$ that is gamma-distributed with shape $\alpha$ and rate $\beta$ is denoted
	$$ X \sim \Gamma(\alpha, \beta) \equiv \operatorname{Gamma}(\alpha, \beta) . $$
	
	The corresponding probability density function in the shape-rate parametrisation is
	$$
		f(x; \alpha, \beta) = 
		\frac{\beta^\alpha x^{\alpha - 1} e^{-\beta x}}{\Gamma(\alpha)}
		\qquad \text{for } x > 0 \quad \alpha, \beta > 0,
	$$
	where $\Gamma(\alpha)$ is the gamma function.
\end{definition}

Let $X_1, X_2, \ldots, X_n$ be $n$ independent and identically distributed random variables following an exponential distribution with rate parameter $\lambda$, then $\sum_i X_i \sim \Gamma(n, 1/\lambda)$ where $n$ is the shape parameter and $1/\lambda$ is the scale.

\begin{definition}{Beta distribution \cite{wikipedia_beta_distribution}}{beta_distribution}
	The probability density function of the \textbf{beta distribution}, for $0 \leq x \leq 1$, and shape parameters $\alpha, \beta > 0$, is a power function of the variable $x$ and of its reflection $(1 - x)$ as follows:
	\begin{align*}
		f(x; \alpha, \beta)
		&= \text{constant} \cdot x^{\alpha - 1} (1 - x)^{\beta - 1} \\
		&= \ddfrac{x^{\alpha - 1} (1 - x)^{\beta - 1}}
			{\int_0^1 u^{a - 1} (1 - u)^{\beta - 1} du} \\
		&= \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}
			x^{\alpha - 1} (1 - x)^{\beta - 1} \\
		&= \frac{1}{B(\alpha, \beta)}
			x^{\alpha - 1} (1 - x)^{\beta - 1} ,
	\end{align*}
	where $\Gamma(z)$ is the gamma function. The beta function, $B$, is a normalisation constant to ensure that the total probability is 1. In the above equations, $x$ is a realisation --- an observed value that actually occurred --- of a random process $X$.

	A random variable $X$ beta-distributed with parameters $\alpha$ and $\beta$ is denoted by
	$$ X \sim \operatorname{Beta}(\alpha, \beta) . $$
\end{definition}

\begin{definition}{Q-Q plot}{qq_plot}
	! quantiles !
\end{definition}